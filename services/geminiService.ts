
import { GoogleGenAI } from "@google/genai";
import { ScenarioType, SimulationConfig, ChatMessage } from "../types";

const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

// Helper to format context for the AI
const getContextPrompt = (scenario: ScenarioType, config: SimulationConfig, timeElapsed: number) => {
  let scenarioDesc = "";
  switch (scenario) {
    case ScenarioType.LINEAR_MEET:
      scenarioDesc = "ç»å…¸ç›¸é‡é—®é¢˜ (ç›´çº¿ï¼Œé¢å¯¹é¢å‡ºå‘)ã€‚";
      break;
    case ScenarioType.LINEAR_CHASE:
      scenarioDesc = "è¿½åŠé—®é¢˜ (ç›´çº¿ï¼ŒåŒå‘è·‘ï¼Œçº¢è¿½è“)ã€‚";
      break;
    case ScenarioType.ROUND_TRIP:
      scenarioDesc = "å¤šæ¬¡å¾€è¿”ç›¸é‡é—®é¢˜ (ä¸¤ç«¯ä¹‹é—´æ¥å›è·‘)ã€‚";
      break;
    case ScenarioType.CIRCULAR:
      scenarioDesc = "ç¯å½¢è·‘é“é—®é¢˜ (å°é—­åœ†åœˆï¼Œå¯èƒ½å¥—åœˆ)ã€‚";
      break;
  }

  return `
    [å½“å‰æ¨¡æ‹ŸçŠ¶æ€æ•°æ®]
    åœºæ™¯ç±»å‹: ${scenarioDesc}
    è·‘é“æ€»é•¿: ${config.trackLength} ç±³ã€‚
    çº¢é˜Ÿ(å…”å­)é€Ÿåº¦: ${config.redSpeed} m/sã€‚
    è“é˜Ÿ(ä¹Œé¾Ÿ)é€Ÿåº¦: ${config.blueSpeed} m/sã€‚
    ${scenario === ScenarioType.LINEAR_CHASE ? `åˆå§‹è¿½å‡»å·®è·: ${config.initialDistance} ç±³ã€‚` : ''}
    å½“å‰æ¨¡æ‹Ÿè¿è¡Œæ—¶é—´: ${timeElapsed.toFixed(1)} ç§’ã€‚
  `;
};

export const chatWithMathTeacher = async (
  currentMessage: string,
  chatHistory: ChatMessage[],
  scenario: ScenarioType,
  config: SimulationConfig,
  timeElapsed: number
): Promise<string> => {
  const modelId = "gemini-2.5-flash";
  
  const contextData = getContextPrompt(scenario, config, timeElapsed);

  const systemInstruction = `
    ä½ æ˜¯ä¸€ä½å¹½é»˜ã€äº²åˆ‡çš„å°å­¦å¥¥æ•°è€å¸ˆã€‚ä½ çš„å­¦ç”Ÿæ˜¯ä¸€ä¸ª10å²çš„å­©å­ã€‚
    ä½ çš„ä»»åŠ¡æ˜¯åˆ©ç”¨æä¾›çš„[å½“å‰æ¨¡æ‹ŸçŠ¶æ€æ•°æ®]æ¥å›ç­”å­¦ç”Ÿçš„é—®é¢˜ã€‚
    
    æ•™å­¦åŸåˆ™:
    1. **ç»“åˆæ•°æ®**: å›ç­”æ—¶å¿…é¡»å¼•ç”¨å½“å‰çš„å…·ä½“æ•°å­—ï¼ˆé€Ÿåº¦ã€è·ç¦»ã€æ—¶é—´ï¼‰ã€‚
    2. **é€šä¿—æ˜“æ‡‚**: ä¸è¦ç”¨å¤æ‚çš„ä»£æ•°å…¬å¼ï¼Œå¤šç”¨ç®—æœ¯æ€ç»´ï¼ˆæ¯”å¦‚â€œé€Ÿåº¦å’Œâ€ã€â€œé€Ÿåº¦å·®â€ï¼‰ã€‚
    3. **ç”ŸåŠ¨æœ‰è¶£**: ä½¿ç”¨è¡¨æƒ…ç¬¦å· (ğŸ°, ğŸ¢, ğŸ, â±ï¸) æ´»è·ƒæ°”æ°›ã€‚
    4. **å¯å‘å¼**: å¦‚æœå­¦ç”Ÿé—®ä¸ºä»€ä¹ˆï¼Œå¼•å¯¼ä»–ä»¬çœ‹å›¾æˆ–æ€è€ƒï¼Œè€Œä¸æ˜¯ç›´æ¥ä¸¢å…¬å¼ã€‚
    5. **ç®€ç»ƒ**: æ¯æ¬¡å›ç­”æ§åˆ¶åœ¨ 100-150 å­—å·¦å³ï¼Œä¸è¦é•¿ç¯‡å¤§è®ºã€‚
  `;

  // Construct a simple prompt flow. 
  // We inject the context strongly in the latest prompt to ensure it uses the LATEST slider values.
  const prompt = `
    ${contextData}
    
    å­¦ç”Ÿä¹‹å‰çš„é—®é¢˜å’Œä½ çš„å›ç­”:
    ${chatHistory.slice(-4).map(m => `${m.role === 'user' ? 'å­¦ç”Ÿ' : 'è€å¸ˆ'}: ${m.text}`).join('\n')}
    
    å­¦ç”Ÿç°åœ¨é—®: "${currentMessage}"
    
    è¯·ä½œä¸ºè€å¸ˆå›ç­” (ç›´æ¥è¾“å‡ºå›ç­”å†…å®¹):
  `;

  try {
    const response = await ai.models.generateContent({
      model: modelId,
      contents: prompt,
      config: {
        systemInstruction: systemInstruction,
      }
    });
    return response.text || "è€å¸ˆæ­£åœ¨æ€è€ƒæ€ä¹ˆè§£é‡Šæ›´ç®€å•ï¼Œè¯·ç¨ç­‰...";
  } catch (error) {
    console.error("Gemini API Error:", error);
    return "è€å¸ˆçš„ç½‘ç»œæœ‰ç‚¹å¡ï¼Œè¯·å†é—®ä¸€æ¬¡è¯•è¯•ï¼";
  }
};
